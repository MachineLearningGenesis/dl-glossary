# Deep learning glossary

Deep learning terminology can be difficult and overwhelming, especially to newcomers. This glossary tries to define the most commonly used terms.

Since terminology is constantly changing with new terms appearing every day this glossary will be in a permanent work in progress. Feel free to suggest new terms using issues and pull requests.

* [Activation function](#activation-function)
* [Attention mechanism](#attention-mechanism)
* [Autoencoder](#autoencoder)
* [Backpropagation](#backpropagation)
* [Batch](#batch)
* [Batch normalization](#batch-normalization)
* [Bias](#bias)
* [Capsule Network](#capsule-network)
* [Convolution Neural Network (CNN)](#cnn)
* [Data augmentation](#data-augmentation)
* [Dropout](#dropout)
* [Epoch](#epoch)
* [Exploding gradient](#exploding-gradient)
* [Feed-forward](#feed-forward)
* [Gradient Recurrent Unit (GRU)](#gru)
* [Graph Convolutional Network (GCN)](#gcn)
* [Generative Adversarial Network (GAN)](#gan)
* [Kernel](#kernel)
* [Learning rate](#learning-rate)
* [Long Short-Term Memory (LSTM)](#lstm)
* [Multi Layer Perceptron (MLP)](#mlp)
* [Pooling](#pooling)
* [Pytorch](#pytorch)
* [Recurrent Neural Network (RNN)](#rnn)
* [Relational reasoning](#relational-reasoning)
* [Residual Networks (ResNet)](#resnet)
* [Siamese Neural Network](#siamese-neural-network)
* [Tensorflow](#tensorflow)
* [Vanishing gradient](#vanishing-gradient)

------

## Activation function
## Attention mechanism
## Autoencoder
## Backpropagation
## Batch
## Batch normalization
## Bias
##Â Capsule Network
## CNN
## Data augmentation
## Dropout
## Epoch
## Exploding gradient
## Feed-forward
## GRU
## GCN
## GAN
## Kernel
## Learning rate
## LSTM
## MLP
## Pooling
## Pytorch
## RNN
## Relational reasoning
## ResNet
## Siamese Neural Network
## Tensorflow
## Vanishing gradient